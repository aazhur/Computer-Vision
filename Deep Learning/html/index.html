<html>
<head>
<title>Computer Vision Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Anastasia Zhurikhina</h1>
</div>
</div>
<div class="container">

<h2> Project 6: Deep Learning </h2>

<div style="float: right; padding: 20px">
<img src="placeholder.png" width="80%"/>
</div>
<h3>Part 0</h3>
<p> For this project we are to train and apply deep convolutional networks for scene recognition. This is another approach compared to project 4, where an accuracy of 60 to 70% could be achieved on a 15-way scene classification with elaborate feature mining and SVM application. In this project, however the deep learning methods are applied. </p>

<ol>
<li>In part 0 we train and apply a simple 2-layer convolutional network to see how it works and try to see the logistics of it.</li>
<li>In part 1 by improving the data augmentation, deepening the network and smoothing and cleaning it by adding Dropout layer and Batch normalization after each convolutional layer, we improve the performance of our simplistic CNN.</li>
<li>In part 2, given the AlexNet we fine-tune the network by deleting and adding a new fc8 layer (Linear, playing around with weights distribution and bias) to achieve the best performance so far (getting out 85% accuracy without any particular data-wizarding is not a challenge with such SNN architecture).</li>
</ol>

<p> DL methods have a certain advantage over ML, especially when presented with a challenge of big data sets. DLNs "try to learn high-level features from data in an incremental manner. This eliminates the need of domain expertise and hard-core feature extraction." <sup> 1 </sup> Although, in part 0 we can see that almost no data augmentation and elimination of certain steps to CNN cannot show a high enough performance, as best accuracy achieved was 35% (Table 1). It is important to note, that by simply adding BatchNorm the performance could be bested already by 10% as "it makes the optimization landscape significantly smoother (Table 2). This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training." <sup> 2 </sup></p>
<table border=0.1>

<tr>
<td>
<img src="1.png" width="24%"/>
<img src="2.png" width="24%"/>
<img src="3.png" width="24%"/>
<img src="4.png" width="24%"/>
</td>
</tr>

</table>

<table border=0.1>

<tr>
<td>
<img src="5.png" width="24%"/>
<img src="6.png" width="24%"/>
<img src="7.png" width="24%"/>
<img src="8.png" width="24%"/>
</td>
</tr>

</table>

<h3>Part 1 </h3>

<p> In part 1, first we had to do the data jittering or data augmentation to improve the CNN training performance. The CNN is a sequential neural network, as we go step by step through layers (sequentially), extracting the features from data and learning to classify them, adjusting the weights at each step through a sequential backpropagation, so we can minimize the loss (negative log likelihood) and thus have a  good accurate mapping of original data.
Data augmentation is an important step, because "when a computer takes an image as an input, it will take in an array of pixel values." <sup> 3 </sup> Therefore, shifting, mirroring, cropping, normalizing, color jittering and small rotations while won't change the image and object in it into something different to a human eye, it would make significant changes for the computer, as the array of numbers representing the image fed to it would become different. It will allow then for a better training of data, as computer will learn to recognize changes made by say 'camera' (Allowing for a better performance compared to Part 0 (Table 3)).
</p>

<table border=0.1>

<tr>
<td>
<img src="17.png" width="24%"/>
<img src="18.png" width="24%"/>
<img src="19.png" width="24%"/>
<img src="20.png" width="24%"/>
</td>
</tr>

</table>

<p> Secondly, we had to add the Dropout layer to the CNN, as to eliminate the problem of overfitting the model. By dropping out a certain number of randomly chosen values we allow the model to show a 'true' performance without the tendency to be perfectly fitted to a training set to a degree it won't be able to recognize any test images. By adding BatchNorm2D after convolutional layers, as mentioned before, we allow for a better adjustment of weights, as a result of smoothness of data, and thus faster training. The convolutional layers allow us for feature extraction, the Maxpool (which pools the max out of what's left after convolutional filter is applied) and Batch added to them allow for the most meaningful and smooth data extracted after the convolving is applied. The kernel size, stride and padding therefore serve the purpose of what exactly we want our features represent in an image, accounting for the images landscape, contrasts, number of details and etc. In this scenario of scene recognition I have decided to choose the first kernel to be smaller than initially proposed and add padding as it increased performance, of course, and I speculated it would be so due to 'little' important details, that might represent scenes like kitchen or mountains, yet I chose a pretty big sliding window for maxpool to take only most robust data out of filtered architecture at the convolutional level, which has preserved the intial volume of data. On the next step I tried to mimic the first one by applying small filter windows and pooling out 'best' values out of them (as I imagine this process to be). I have also played with number of channels, more channels did indeed show more robustness of data, yet for time saving purposes I've decided to not go above 20, as 30 channels would help to increase accuracy by a 1% or so, it wouldn't deliver a significant increase of performance, yet take quite some time to finish. The Relu layers for nonlinearity introduction to otherwise linearly extracted data were also added after each Conv except the last.
All this CNN and data wizarding helped to increase the performance by 30% compared to part 0. Leading to a result of 65% (Table 4).
</p>

<table border=0.1>

<tr>
<td>
<img src="9.png" width="24%"/>
<img src="10.png" width="24%"/>
<img src="11.png" width="24%"/>
<img src="12.png" width="24%"/>
</td>
</tr>

</table>

<h3>Part 2</h3>
<p> Lastly, we had to embrace "Strategy B to fine-tune an existing AlexNet network. We took an existing network, replaced the final layer (or more) with random weights, and trained the entire network again with images and ground truth labels for the recognition task. Thus effectively treating the pre-trained deep network as a better initialization than the random weights used when training from scratch". Here, we added a linear layer with normally distributed weights and a nice, yet not to strict, threshold for bias. Which has allowed to easily achieve the desired threshold of 85% (5% above the proposed performance cut, Table 5). </p>
<table border=0.1>

<tr>
<td>
<img src="13.png" width="24%"/>
<img src="14.png" width="24%"/>
<img src="15.png" width="24%"/>
<img src="16.png" width="24%"/>
</td>
</tr>

</table>

<div style="clear:both" >

	<h2> Citations </h2>

	<p> <sup> 1 </sup> 953383504824263. (2018, March 21). Why Deep Learning over Traditional Machine Learning? Retrieved from https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063 </p>
	<p> <sup> 2 </sup> arXiv:1805.11604 [stat.ML] </p>
	<p> <sup> 3 </sup> Deshpande, A. (n.d.). A Beginners Guide To Understanding Convolutional Neural Networks Part 2. Retrieved from https://adeshpande3.github.io/adeshpande3.github.io/A-Beginners-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/</p>
	<p> <sup> 4 </sup> James Hays, Computer Vision CS 6476 Fall 2018 slides </p>
	<p> <sup> 5 </sup> Szeliski, R. (2011). Computer vision algorithms and applications. London: Springer. </p>

</div>
</body>
</html>
